# Configuration version (required)
version: 1.1.6

# Cache settings: Set to true to enable caching
cache: true

custom:
    - name: "Lite LLM"
      # A place holder - otherwise it becomes the default (OpenAI) key
      # Provide the key instead in each "model" block within "litellm/litellm-config.yaml"
      apikey: "${LITELLM_API_KEY}"
      # See the required changes above in "Start LiteLLM Proxy Server" step.
      baseURL: "http://host.docker.internal:4000"
      # A "default" model to start new users with. The "fetch" will pull the rest of the available models from LiteLLM
      # More or less this is "irrelevant", you can pick any model. Just pick one you have defined in LiteLLM.
      models:
        default: ["gpt-4o-mini"]
        fetch: true
      titleConvo: true
      titleModel: "gpt-4o-mini"
      summarize: false
      summaryModel: "gpt-3.5-turbo"
      forcePrompt: false
      modelDisplayLabel: "Lite LLM"
      
# See the Custom Configuration Guide for more information:
# https://docs.librechat.ai/install/configuration/custom_config.html
